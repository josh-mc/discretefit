---
title: "Randomly Sampling Contingency Tables with Fixed Margins"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tables_fixed_margins}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(discretefit)
library(ggplot2)
library(dplyr)

deal <- function(x, y) {
  
  cts <- 0
  
  while(cts <= y) {
  
    for(i in 1:length(x)) {
    
      if(cts <= y) {
      
        x[i] = x[i] + 1
      
        cts <- cts + 1
      
      }
    }
  
    }
  
  return(x)
  
}
```

```{r}

col <- c(7, 4, 4, 5, 6)
row <- c(2, 3, 4, 5, 6)

n <- c(10, 20, 30, 50, 100, 200, 500, 1000)

n_all <- length(n) * length(col)

rcont2_speed <- numeric(n_all)
rcont3_speed <- numeric(n_all)
total_n <- numeric(n_all)
total_rows <- numeric(n_all)
total_cols <- numeric(n_all)
iterations_n <- numeric(n_all)

cnt <- 1
number_iterations <- 1000


for(j in 1:length(col)) { 

  for(i in n) {
  
  
  rsums <- deal(numeric(row[j]), i)
  csums <- deal(numeric(col[j]), i)

  speed <- bench::mark(min_iterations = number_iterations, 
                     check = FALSE,
                     rcont2 = r2dtable(1000, rsums, csums),
                     rcont3 = rcont3_cpp(1000, csums, rsums, seed = sample.int(.Machine$integer.max, 1))
                     )

rcont2_speed[cnt] <- speed$median[1]
rcont3_speed[cnt] <- speed$median[2]
total_n[cnt] <- i
total_cols[cnt] <- col[j]
total_rows[cnt] <- row[j]
iterations_n <- number_iterations

cnt <- cnt + 1

  }
}

```

Patefield compared run time for RCONT and RCONT2 for forty contingency tables of varying dimensions and varying sample sizes with uniform (or as near uniform as possible) row and column totals. 1,000 samples of each table were generated. For the 2 x 7 and 3 x 4 tables, RCONT2 was faster than RCONT for all sample sizes above ten. For the remaining tables, RCONT2 was faster than RCONT for all sample sizes above 100. Often RCONT2 was substantially faster. For example, for the 2 x 7 table with a sample size of 1,000, RCONT2 was over 30 times faster than RCONT. 

Taking the same forty contingency tables, 1,000 samples of each table were generated by RCONT2, as implemented in the R `stats` package function `r2dtables` and by RCONT3. This experiment was repeated 1,000 times and median times were recorded using the `bench` packages. 

In one case, that of the 3 x 4 contingency table with a sample of 10, run times for RCONT2 and RCONT3 were comparable. For all other thirty-nine cases, RCONT3 was faster. On average, RCONT3 was more than three times faster than RCONT2. 

```{r}
dat <- data.frame(total_n, total_cols, total_rows, rcont2_speed, rcont3_speed) %>%
  dplyr::mutate(ratio = rcont2_speed / rcont3_speed,
                title = paste(paste(total_rows, "x"), total_cols))

dat %>%
  ggplot(aes(total_n, ratio)) +
  geom_point() +
  geom_line() +  
  facet_wrap(~title) +
  ggtitle("Ratio of runtime for RCONT2 to RCONT3 for sampling 1,000 tables") + 
  theme_minimal()
```

```{r}
mean(dat$ratio)
```
Patefield notes that RCONT2 is increasingly more efficient than RCONT as the sample size increases but the size of the table is held constant. This is also true for RCONT3. For smaller tables (2 x 7 and 3 x 4), RCONT2 is faster than RCONT3 as soon as the sample size is increased to 2,000. For the largest table (6 x 6), RCONT3 is still faster than RCONT2 with a sample size of 10,000.

(For only 10 iterations)

```{r}

col <- c(7, 4, 4, 5, 6)
row <- c(2, 3, 4, 5, 6)

n <- c(2000, 5000, 10000)

n_all <- length(n) * length(col)

rcont2_speed <- numeric(n_all)
rcont3_speed <- numeric(n_all)
total_n <- numeric(n_all)
total_rows <- numeric(n_all)
total_cols <- numeric(n_all)
iterations_n <- numeric(n_all)

cnt <- 1
number_iterations <- 10


for(j in 1:length(col)) { 

  for(i in n) {
  
  
  rsums <- deal(numeric(row[j]), i)
  csums <- deal(numeric(col[j]), i)

  speed <- bench::mark(min_iterations = number_iterations, 
                     check = FALSE,
                     rcont2 = r2dtable(1000, rsums, csums),
                     rcont3 = rcont3_cpp(1000, csums, rsums, seed = sample.int(.Machine$integer.max, 1))
                     )

rcont2_speed[cnt] <- speed$median[1]
rcont3_speed[cnt] <- speed$median[2]
total_n[cnt] <- i
total_cols[cnt] <- col[j]
total_rows[cnt] <- row[j]
iterations_n <- number_iterations

cnt <- cnt + 1

  }
}

```

```{r}
dat <- data.frame(total_n, total_cols, total_rows, rcont2_speed, rcont3_speed) %>%
  dplyr::mutate(ratio = rcont2_speed / rcont3_speed,
                title = paste(paste(total_rows, "x"), total_cols))

dat %>%
  ggplot(aes(total_n, ratio)) +
  geom_point() +
  geom_line() +  
  facet_wrap(~title) +
  ggtitle("Ratio of runtime for RCONT2 to RCONT3 for sampling 1,000 tables") + 
  theme_minimal()
```
For larger, relatively sparse contingency tables, RCONT3 was much faster than RCONT2. For the tables and sample sizes below, RCONT3 was, on average, over sixteen times faster than RCONT2. 

(For only 10 iterations)

```{r}

col <- c(10, 25, 50, 100)
row <- c(10, 25, 50, 100)

n <- c(250, 500, 1000, 2000)

n_all <- length(n) * length(col)

rcont2_speed <- numeric(n_all)
rcont3_speed <- numeric(n_all)
total_n <- numeric(n_all)
total_rows <- numeric(n_all)
total_cols <- numeric(n_all)
iterations_n <- numeric(n_all)

cnt <- 1
number_iterations <- 10


for(j in 1:length(col)) { 

  for(i in n) {
  
  
  rsums <- deal(numeric(row[j]), i)
  csums <- deal(numeric(col[j]), i)

  speed <- bench::mark(min_iterations = number_iterations, 
                     check = FALSE,
                     rcont2 = r2dtable(1000, rsums, csums),
                     rcont3 = rcont3_cpp(1000, csums, rsums, seed = sample.int(.Machine$integer.max, 1))
                     )

rcont2_speed[cnt] <- speed$median[1]
rcont3_speed[cnt] <- speed$median[2]
total_n[cnt] <- i
total_cols[cnt] <- col[j]
total_rows[cnt] <- row[j]
iterations_n <- number_iterations

cnt <- cnt + 1

  }
}

```

```{r}
dat <- data.frame(total_n, total_cols, total_rows, rcont2_speed, rcont3_speed) %>%
  dplyr::mutate(ratio = rcont2_speed / rcont3_speed,
                title = paste(paste(total_rows, "x"), total_cols))

dat %>%
  ggplot(aes(total_n, ratio)) +
  geom_point() +
  geom_line() +  
  facet_wrap(~title) +
  ggtitle("Ratio of runtime for RCONT2 to RCONT3 for sampling 1,000 tables") + 
  theme_minimal()
```

```{r}
mean(dat$ratio)
```
